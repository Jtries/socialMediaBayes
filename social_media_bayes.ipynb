{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-Stealed, or how social medias are stealing our lives\n",
    "\n",
    "_Year 2025, a great portion of our lives is experienced within a virtual environment where we can access an infinite stream of information that excites our brain and that pleasures it with rivers of dopamine. The speed, the bright colors, the catchy audios and the enraging news, everything has been carefully engineered to keep us trapped in front of a glowy screen_\n",
    "\n",
    "Told like this it might sound like the incipit of a cyberpunk novel, but it's actually not that far from our reality, is it? The virtual environments I'm talking about are obviously social medias, that despite all they've done for the free circulation of information, have a scary dark side.\n",
    "\n",
    "The aim of this project is to shed a light on that side and investigate where does our time go and why it seems so difficult to put a stop to a now far too common habbit: doomscrolling.\n",
    "\n",
    "But why should social medias be implemented to keep us on their platforms? To understand this, it's important to clarify what is their business model. Social medias' profit originates primarely from the \"attention economy\", in other words, they make money by showing tertiary companies' adds to their user base (and by selling their data, but that is a whole other story). So, when the content you're consuming is suddently interrupted by an ad, the attention economy has completed its circle: a company made its attempt to convince you to buy their product and the hosting platform made its share. The follow-up question is then, how does the social media of the situation boosts its revenue? Simple, it tries to maximise the exposure time and the engagement of the user. And how does it do it? With a plethora of sophisticated [psychological tricks](https://www.youtube.com/watch?v=uaaC57tcci0) that hook the user's attention for [hours](https://www.statista.com/statistics/433871/daily-social-media-usage-worldwide/) on end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pgmpy.models import DiscreteBayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.estimators import PC, HillClimbSearch, TreeSearch\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes the given model with the given title\n",
    "def visualize_model(model, title):\n",
    "    graph = nx.DiGraph(model.edges)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    nx.draw_circular(G=graph, with_labels=True, node_size=4000, node_color=\"#8621ff\", arrowsize=20, font_size=8)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzes the structure of the Bayesian Network\n",
    "def analyze_model(model, model_name):\n",
    "    print(model_name.upper())\n",
    "    print(f\"\\nD-separations: {model.get_independencies().get_assertions()}\")\n",
    "    print(f\"\\nImmoralities: {model.get_immoralities()}\") # TODO should I include ALL v-structures?\n",
    "    for n in model.nodes:\n",
    "        print(f\"\\n{n}\")\n",
    "        print(f\"Local semantics: {model.local_independencies(n)}\")\n",
    "        print(f\"Markov blanket: {n} is {model.get_markov_blanket(n)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset\n",
    "This project leverages bayesian networks to extract interesting patterns from the dataset [Dark Side Of Social Media](https://www.kaggle.com/datasets/muhammadroshaanriaz/time-wasters-on-social-media?resource=download). This dataset has been generated via synthetic data generation tecniques to simulate real-world social media usage patterns (read the [EDA](https://www.kaggle.com/code/waqi786/in-depth-analysis-of-time-wasters-on-social-media) for further details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads and previews the dataset and its shape\n",
    "data = pd.read_csv('Time-Wasters on Social Media.csv', delimiter=',')\n",
    "print(f'Shape: {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayesian network\n",
    "\n",
    "### 2.1 Variables\n",
    "The dataset contains many variables spanning from the characteristics of the users to the features of their device. However, for the purpose of this project - which focuses on the time spent on social medias due to their addictive traits - only 10 out of 31 columns have been selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selects only relevant columns\n",
    "selected_data = data[['Watch Reason', 'Platform', 'Importance Score', 'Self Control', 'Addiction Level', 'Engagement', 'Scroll Rate', 'Total Time Spent', 'Satisfaction', 'ProductivityLoss']]\n",
    "\n",
    "# Renames the selected columns for clarity\n",
    "selected_data.loc[:, 'ProductivityLoss'].apply(lambda x: 10 - x)\n",
    "selected_data.columns = ['Reason', 'Platform', 'Importance', 'Intentionality', 'Addiction', 'Engagement', 'Context-Switch', 'Time', 'Satisfaction', 'Productivity']\n",
    "selected_data.index = data.UserID.values\n",
    "selected_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a brief descriprion of them:\n",
    "\n",
    "- Reason: the reason why the users entered the platform. It's interesting because it represents the initial emotional state of the users and what they wish to get out of their social media consumption\n",
    "\n",
    "- Platform: the social media platform chosen by the users. It's interesting because each platform has its unique features and implementation\n",
    "\n",
    "- Importance: the importance assigned by the users to the content. It's interesting because it returns relevant information about the cost/benefit tradeoff accepted by the users\n",
    "\n",
    "- Intentionality: the intentionality level that lead the users to enter the platform. It's interesting because...\n",
    "\n",
    "- Addiction: the addiction level of the users to social medias. It's interesting because...\n",
    "\n",
    "- Engagement: the engagement level of the users with the content. It's interesting because it quantifies the degree of activity/passivity of the content consumption experience\n",
    "\n",
    "- Context-Switch: how quickly the users switch from one piece of content to the next. It's interesting because it tells about the width of the users' attention span\n",
    "\n",
    "- Time: the total amount of time that the users have spent on the platform. It's the interesting because...\n",
    "\n",
    "- Satisfaction: the satisfaction level of the users at the end of the content consumption. It's interesting because it returns relevant information about the cost/benefit tradeoff accepted by the users\n",
    "\n",
    "- Productivity: the productivity level of the users during the content consumption. It's interesting because...\n",
    "\n",
    "it's one of the behavioural traits that the study wishes to capture\n",
    "It's the most interesting aspect of the study given that it's the \"price\" of social media consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Correlation matrix\n",
    "Before building the Bayesian Network it can be usefull to look at the correlation matrix to gather some information about the relationships between the selected variables. Given that the correlation matrix only works with numerical variables, the categorical ones have been label-encoded, meaning that each string-value that a variable could take has been univocally mapped into an integer. This encoding has been preferred to the one-hot-encoding for readibility reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodes the categorical columns with a label-encoder\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = selected_data.select_dtypes(include='object').columns\n",
    "encoded_data = selected_data.copy()\n",
    "for c in categorical_columns:\n",
    "    encoded_data[c] = label_encoder.fit_transform(selected_data[c])\n",
    "\n",
    "# Computes the correlation matrix\n",
    "correlation_matrix = encoded_data.corr(method='pearson')\n",
    "\n",
    "# Visualizes the correlation matrix\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=sns.diverging_palette(316, 267, as_cmap=True), fmt=\".2f\", linewidths=.5, mask=mask)\n",
    "plt.title('Correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can already make three considerations:\n",
    "1. most variables are poorly correlated of eachother, so they are basically independent\n",
    "2. some variables (Addiction-Intentionality, Productivity-Satisfaction) are inversly correlated of eachother, so they represent basically the same concept\n",
    "3. some variables (Satisfaction, Productivity, Intentionality, Addiction) are interestingly correlated of eachother but also unexpectedly isolated from other ones\n",
    "\n",
    "**Personal note**: Remember that the dataset used was synthetically produced and **I suspect that for that reason it might not precisely reflect reality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Structure learning and analysis\n",
    "Structure learning is a data-driven process to estimate the structure of a Bayesian Network from the available data. For this project both the constraint-based and score-based approach have been put to the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 PC algorithm (constraint-based)\n",
    "This Bayesian Network was obtained by applying the PC algorithm. This tecnique inserts one variable at a time and adds a link between the new variable and the old ones only if they are conditionally dependent of eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimates the structure of the Bayesian Network from the selected data\n",
    "pc_estimator = PC(selected_data)\n",
    "pc_model = pc_estimator.estimate(show_progress=False)\n",
    "visualize_model(model=pc_model, title=\"PC model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(model=pc_model, model_name=\"PC model\") # TODO fix this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Tree search (score-based)\n",
    "This Bayesian Network was obtained by applying the Tree Search algorithm. This tecnique uses the Chow-Liu algorithm and the TAN algorithm to estimate a tree-like structure from the selected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_estimator = TreeSearch(selected_data, root_node='Intentionality')\n",
    "ts_model = ts_estimator.estimate(show_progress=False)\n",
    "visualize_model(model=ts_model, title=\"Tree search model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(model=ts_model, model_name=\"Tree search model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO interpret the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Structure definition and analysis\n",
    "This Bayesian Network has been build by adding variables one by one following this order:\n",
    "\n",
    "TODO update this section\n",
    "\n",
    "Reason -> Platform -> Importance -> Intentionality -> Engagement -> Context-Switch -> Time -> Satisfaction\n",
    "\n",
    "This is because users choose the platform according to the reason that motivates them to enter the social media space. Depending on the importance of their session, they are more or less intentional with their consumption which effects their engagement and their context-switch. All the previous variables impact the time spent on the platform and the satisfaction associated to the experience.\n",
    "\n",
    "At each step, a link between the new variable and the old ones is added according to these considerations:\n",
    "\n",
    "- Addition of Reason\n",
    "- Addition of Platform\n",
    "    - Reason -> Platform: the platform chosen for the session depends on the reason that initiated it as different platforms might be chosen for different reasons\n",
    "- Addition of Importance\n",
    "    - Reason -> Importance: the importance of the session depends on the reason that initiated it, for ex: the educational purpose might be considered more important that the procrastination one\n",
    "- Addition of Intentionality\n",
    "    - Reason -> Intentionality: the intentionality of the session depends on the reason that initiated it, for ex: if the session started as a habit, then the intentionality might be low, whereas if it started to communicate with someone, then it might be high\n",
    "    - Platform -> Intentionality: the intentionality of the session depends on the platform as certain platforms are more addictive than others\n",
    "- Addition of Engagement\n",
    "    - Reason -> Engagement: the engagement of the session depends on the reason that initiated it, for ex: if the session started for boredom, then the engagement might be low, whereas if it started for entertainment, then it might be high\n",
    "- Addition of Context-Switch\n",
    "    - Platform -> Context-Switch: the context-switch level of the session depends on the chosen platform as certain platforms host short pieces of content while others longer ones\n",
    "    - Engagement -> Context-Switch: the context-switch level of the session depends on the engagement because the more the users are engaged with a certain piece of content the less they context-switch, and vice-versa\n",
    "- Addition of Time\n",
    "    - Intentionality -> Time: the time spent on the platform depends on the intentionality as an intentional session might be more efficient than an unintentional one\n",
    "- Addition of Satisfaction\n",
    "    - Importance -> Satisfaction, Time -> Satisfaction: the satisfaction associated to the session depends on the ratio between its importance and its duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the BN\n",
    "custom_model = DiscreteBayesianNetwork([\n",
    "    ('Reason', 'Platform'),\n",
    "    ('Reason', 'Importance'),\n",
    "    ('Reason', 'Intentionality'),\n",
    "    ('Reason', 'Engagement'),\n",
    "    ('Platform', 'Intentionality'),\n",
    "    ('Platform', 'Context-Switch'),\n",
    "    ('Importance', 'Satisfaction'),\n",
    "    ('Intentionality', 'Time'),\n",
    "    ('Engagement', 'Context-Switch'),\n",
    "    ('Time', 'Satisfaction')\n",
    "])\n",
    "\n",
    "# Fits the data into the BN to learn the CPTs (a CPT is a table that specifies the probability of each value of a variable for every combination of values of its parents)\n",
    "custom_model.cpds = []\n",
    "custom_model.fit(selected_data)\n",
    "\n",
    "# The printing of the CPTs has been commented out as it took too much space\n",
    "# # Prints the CPTs\n",
    "# for cpd in custom_model.get_cpds():\n",
    "#     print('CPT of {}'.format(cpd.variable))\n",
    "#     print(cpd, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(custom_model, \"Custom model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_model(custom_model, \"Custom model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inferences\n",
    "\n",
    "The inferences will investigate low/high levels of *place_holder*, so it is necessary to define some thresholds:\n",
    "\n",
    "- Important session: > 5 (assuming the scale is 1-10)\n",
    "- High intentionality: > 5 (assuming the scale is 1-10)\n",
    "- High engagement: > 500 (assuming the scale is 1-1000)\n",
    "- High context-switch: > 50 (assuming the scale is 1-100)\n",
    "- Long session: > 30 (assuming min as the unit of measurement)\n",
    "- High satisfaction: > 5 (assuming the scale is 1-10)\n",
    "\n",
    "Assumptions on scales and units of measurement are necessary as the dataset description lacks the needed details. However, it was possible to extract the following information about the domains of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the domain of each variable:\n",
    "for c in selected_data.columns:\n",
    "    domain = selected_data[c].unique()\n",
    "    if domain.dtype == \"int64\":\n",
    "        print(f\"Domain of {c}: [min: {domain.min()}, max: {domain.max()}]\")\n",
    "    else:\n",
    "        print(f\"Domain of {c}: {domain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_eliminator = VariableElimination(custom_model)\n",
    "\n",
    "long_time_states = [s for s in custom_model.states['Time'] if s > 30]\n",
    "low_intentionality_states = [s for s in custom_model.states['Intentionality'] if s <= 5]\n",
    "low_satisfaction_states = [s for s in custom_model.states['Satisfaction'] if s <= 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 What is the probability that a shallow session is long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO do some benchmarks\n",
    "# TODO answer the same question with an approximate inference\n",
    "\n",
    "# P(T>30,In<=5)\n",
    "p_d_of_t_and_in = variable_eliminator.query(variables=['Time', 'Intentionality'], joint=True)\n",
    "p_of_long_t_and_low_in = 0\n",
    "for t in long_time_states:\n",
    "    for i in low_intentionality_states:\n",
    "        p_of_long_t_and_low_in += p_d_of_t_and_in.get_value(**{'Time':t, 'Intentionality':i})\n",
    "print(f\"P(T>30,In<=5): {p_of_long_t_and_low_in}\")\n",
    "\n",
    "# P(In<=5)\n",
    "p_d_of_in = variable_eliminator.query(variables=['Intentionality'])\n",
    "p_of_low_in = 0\n",
    "for i in low_intentionality_states:\n",
    "    p_of_low_in += p_d_of_in.get_value(**{'Intentionality':i})\n",
    "print(f\"P(In<=5): {p_of_low_in}\")\n",
    "\n",
    "# P(T>30|T<=5) = P(T>30,In<=5) / P(In<=5)\n",
    "p_of_long_t_given_low_in = p_of_long_t_and_low_in / p_of_low_in\n",
    "print(f\"P(T>30|In<=5): {p_of_long_t_given_low_in}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 What is the probability that a long session is not satisfying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(S<=5,T>30)\n",
    "p_d_of_s_and_t = variable_eliminator.query(variables=['Satisfaction', 'Time'], joint=True)\n",
    "p_of_low_s_and_long_t = 0\n",
    "for d in low_satisfaction_states:\n",
    "    for t in long_time_states:\n",
    "        p_of_low_s_and_long_t += p_d_of_s_and_t.get_value(**{'Satisfaction':d, 'Time':t})\n",
    "print(f\"P(S<=5,T>30): {p_of_low_s_and_long_t}\")\n",
    "\n",
    "# P(T>30)\n",
    "p_d_of_t = variable_eliminator.query(variables=['Time'])\n",
    "p_of_long_t = 0\n",
    "for t in long_time_states:\n",
    "    p_of_long_t += p_d_of_t.get_value(**{'Time':t})\n",
    "print(f\"P(T>30): {p_of_long_t}\")\n",
    "\n",
    "# P(S<=5|T>30) = P(S<=5,T>30) / P(T>30)\n",
    "p_of_low_s_given_long_t = p_of_low_s_and_long_t / p_of_long_t\n",
    "print(f\"P(S<=5|T>30): {p_of_low_s_given_long_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 What is the probability that a long and shallow session is not satisfying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(S<=5,T>30,In<=5)\n",
    "p_d_of_s_and_t_and_in = variable_eliminator.query(variables=['Satisfaction', 'Time', 'Intentionality'], joint=True)\n",
    "p_of_low_s_and_long_t_and_low_in = 0\n",
    "for d in low_satisfaction_states:\n",
    "    for t in long_time_states:\n",
    "        for i in low_intentionality_states:\n",
    "            p_of_low_s_and_long_t_and_low_in += p_d_of_s_and_t_and_in.get_value(**{'Satisfaction':d, 'Time':t, 'Intentionality':i})\n",
    "print(f\"P(S<=5,T>30,In<=5): {p_of_low_s_and_long_t_and_low_in}\")\n",
    "\n",
    "# P(S<=5|T>30,In<=5) = P(S<=5,T>30,In<=5) / P(T>30,In<=5)\n",
    "p_of_low_s_given_long_t_and_low_in = p_of_low_s_and_long_t_and_low_in / p_of_long_t_and_low_in\n",
    "print(f\"P(S<=5|T>30,In<=5): {p_of_low_s_given_long_t_and_low_in}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 What is the probability that a shallow session is not satisfying?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(S<=5,In<=5)\n",
    "p_d_of_s_and_in = variable_eliminator.query(variables=['Satisfaction', 'Intentionality'], joint=True)\n",
    "p_of_low_s_and_low_in = 0\n",
    "for d in low_satisfaction_states:\n",
    "    for i in low_intentionality_states:\n",
    "        p_of_low_s_and_low_in += p_d_of_s_and_in.get_value(**{'Satisfaction':d, 'Intentionality':i})\n",
    "print(f\"P(S<=5,In<=5): {p_of_low_s_and_low_in}\")\n",
    "\n",
    "# P(S<=5|In<=5) = P(S<=5,In<=5) / P(In<=5)\n",
    "p_of_low_s_given_low_in = p_of_low_s_and_low_in / p_of_low_in\n",
    "print(f\"P(S<=5|In<=5): {p_of_low_s_given_low_in}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
